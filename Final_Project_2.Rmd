---
title: "Data 621 Final Project"
author: "(Group 4) Eddie Xu, Mohamed Hassan-El Serafi, Chun Shing Leung, Keith Colella, Yina Qiao"
date: "`r Sys.Date()`"
output: html_document
---

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(mlogit)
library(ggplot2)
library(cowplot)
library(caret)
library(MASS)
library(randomForest)
library(car)
library(pROC)
library(reshape2)
library(visdat)
main
library(pscl)

main
```

The data set is from Kaggle:
https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset


```{r}
mydata<- read.csv("https://raw.githubusercontent.com/tonyCUNY/DATA_621/refs/heads/main/heart.csv")
```

## EDA - Exploratory Data Analysis

Attribute Information:
age
sex
chest pain type (4 values)
resting blood pressure
serum cholestoral in mg/dl
fasting blood sugar > 120 mg/dl
resting electrocardiographic results (values 0,1,2)
maximum heart rate achieved
exercise induced angina
oldpeak = ST depression induced by exercise relative to rest
the slope of the peak exercise ST segment
number of major vessels (0-3) colored by flourosopy
thal: 0 = normal; 1 = fixed defect; 2 = reversable defect

```{r}
# Descriptive statistics
str(mydata)
```

```{r}
#Transform the variables as factor and numeric
mydata.clean <- mydata %>%
  mutate(
    sex = factor(ifelse(sex == 0, "F", "M")),
    age = as.numeric(age),
    trestbps = as.numeric(trestbps),
    chol = as.numeric(chol),
    thalach = as.numeric(thalach),
    cp = as.factor(cp),
    fbs = as.factor(fbs),
    restecg = as.factor(restecg),
    exang = as.factor(exang),
    slope = as.factor(slope),
    ca = as.factor(as.integer(ca)),
    thal = as.factor(as.integer(thal)),
    target = factor(ifelse(target == 0, "No_Disease", "Disease"))
  )

```


```{r}
str(mydata.clean)
```





```{r}
# Statistical summary
summary(mydata.clean)
```
## Checking for Missing Value

1. No Missing Value for both numeric and Categorical Variables

```{r, echo=FALSE}
num_vars <- mydata.clean %>% select_if(where(is.numeric))
vis_miss(num_vars, cluster = TRUE) + 
  ggtitle("Numeric Variables - Most Missing Values") +
  theme(
    plot.title = element_text(face = "bold"),
    plot.margin = unit(c(1, 2, 1, 1), "cm")
  )
```

```{r, echo=FALSE}
cat_vars <- mydata.clean %>% select_if(~ is.factor(.))
cat_vars <- cat_vars %>% 
  mutate(across(everything(), ~na_if(as.character(.), "")))
vis_miss(cat_vars, cluster = TRUE) +
  ggtitle("Categorical Variables - Most Missing Values") +
  theme(
    plot.title = element_text(face = "bold"),
    plot.margin = unit(c(1, 2, 1, 1), "cm")
  )
```

## Box plot

```{r, warning=FALSE}

cat_vars%>%
  gather() %>%
  ggplot(aes(value)) +
  geom_bar(fill = "lightblue", color="grey") +
  facet_wrap(~ key, scales = "free", ncol = 4) +
  theme(
    panel.grid = element_blank(), 
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  labs(title = "Bar Plots of Categorical Variables")

```


## Correlation Matrix


A correlation matrix plot is a visual representation of the pairwise correlation coefficients between variables in your dataset. The values in the matrix represent the correlation coefficient (often Pearson’s correlation) between two variables. 

Interpret the Values:

High Positive Correlation (e.g., 0.8 or 0.9): Strong positive linear relationship between the two variables. For example, age and cholesterol levels might have a high positive correlation.

Moderate Correlation (e.g., 0.4 to 0.6): A moderate positive relationship. These variables are related, but not perfectly.

Low or No Correlation (e.g., 0.1 to 0.3 or close to 0): Weak or no linear relationship. These variables are not strongly associated.

Negative Correlation (e.g., -0.3 to -0.9): A negative relationship, where an increase in one variable corresponds to a decrease in the other.



```{r vif_model, warning=FALSE, message=FALSE}
cor_matrix <- cor(mydata.clean %>% select_if(where(is.numeric)), use = "complete.obs")

cor_long <- melt(cor_matrix)
cor_long <- cor_long[as.numeric(cor_long$Var1) > as.numeric(cor_long$Var2), ]


ggplot(cor_long, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = ifelse(value != 0, round(value, 2), "")), 
            color = "black", size = 3, face="bold") +  # Show only significant values
  scale_fill_gradient2(low = "pink", high = "blue", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), 
                       space = "Lab", name = "Correlation") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 10),  # Adjust x-axis label
    axis.text.y = element_text(size = 10),                                   # Adjust y-axis label
    axis.title = element_blank(),                                            # Remove axis titles
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14)          # Center plot title
  ) +
  ggtitle("Correlation Matrix")
```



##  VIF Check

the VIF score analysis is conducted to check for any multicollinearity.

```{r vif_model, warning=FALSE, message=FALSE}
# fit a linear regression before VIF score
vif_model_all <- lm(as.numeric(target) ~ ., data = mydata.clean)

summary(vif_model_all)
```

After the model fitting, 

The following variables has P-value larger than 0.05, showing that predictor variables may not be significantly associated with the outcome

age
fbs1        
slope1
restecg2

```{r vif_score, warning=FALSE, message=FALSE}
# perform VIF
vif_value = vif(vif_model_all)
vif_value
```




## Data Balance 
1. Target is fairly balanced (Almost 50-50% with no disease/disease)
2. Each level of categorical variables are represented by lots of patients
3. We can conclude the data is not imbalanced

```{r}

summary_data <- mydata.clean %>%
  group_by(target) %>%
  summarise(sum_count = n()) %>%
  mutate(percentage = (sum_count / sum(sum_count)) * 100)


ggplot(summary_data, aes(x = target, y = sum_count, fill = target)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5), size = 5, color = "white") + 
  labs(title = "Sum Count of Target with Percentages", 
       x = "Target Value", 
       y = "Sum Count") +
  theme_minimal()
```


```{r}
# 
variables <- c("sex", "cp", "fbs", "restecg", "slope", "ca")
plot_data <- lapply(variables, function(var) {
  mydata.clean |> 
    group_by(across(all_of(var)), target) |> 
    summarise(count = n(), .groups = "drop") |> 
    mutate(variable = var, level = get(var))
}) %>%
  bind_rows()

# Create the faceted bar plot
ggplot(plot_data, aes(x = level, y = count, fill = factor(target))) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~variable, scales = "free_x") +
  labs(
    title = "Relationship between Target and Other Variables",
    x = "Variable Levels",
    y = "Count",
    fill = "Target"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Splitting the data

```{r}
# Partition data - train (80%) - test(20%)
set.seed(123)
indexSet <-sample(2, nrow(mydata.clean), replace = T, prob = c(0.8, 0.2))
train <- mydata.clean[indexSet==1,]
test <- mydata.clean[indexSet==2,]

```



## Model 1 -  Logistic Regression

```{r}

main
logistic_full <- glm(target ~., data=train, family = "binomial")
summary(logistic_full)

```

##  VIF Check

VIF quantifies how much the variance of a regression coefficient is inflated due to multicollinearity with other predictors in the model. A high VIF indicates that a predictor is highly correlated with other predictors, meaning it might be redundant.

Multicollinearity in logistic regression can cause inflated standard errors for the coefficients, making them unreliable. This can lead to problems with statistical significance testing and interpretation.
High multicollinearity means that the model cannot determine the individual effect of correlated predictors, and you may end up with misleading results.

Interpretation of VIF Values:
VIF = 1: No correlation with other predictors (ideal).
1 < VIF < 5: Moderate correlation; not a problem in most cases.
VIF > 5 or 10: High correlation with other predictors; indicates potential multicollinearity issues. It suggests that you should consider removing or combining predictors.

```{r}
vif(logistic_full)
```

## Feature Selection - Improve the model with StepAIC

AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) are generally better choices than Pseudo's R-squared.

AIC vs. BIC
AIC (Akaike Information Criterion):

Focuses on finding the model that best balances goodness-of-fit and complexity.
It penalizes the inclusion of extra predictors, but less strictly than BIC.
Use AIC if prediction accuracy is the primary goal, as it tends to favor slightly more complex models.

BIC (Bayesian Information Criterion):

Similar to AIC but applies a stronger penalty for model complexity, especially with larger sample sizes.
Use BIC if the focus is on selecting a parsimonious model (simpler with fewer predictors) or if your dataset is large.

We use AIC because our goal is prediction and we have more variables in this model. Step AIC function suggests us remove two variables: fbs, restecg.(which gives us the lowest AIC score after removing them)

logistic_full <- glm(target ~., data=mydata.clean, family = "binomial")
summary(logistic_full)

```
## Improve the model with StepAIC

1. fbs, restecg are removed.


```{r}

logistic_2 <- stepAIC(logistic_full)
summary(logistic_2)

```


# Logistic Regression Coefficient (logit Model)
- The coefficient differ by a scale factor and therefore we cannot interpret the magnitude of the coefficients
- coefficient interpretation: 

    Individual who are male, higher resting blood pressure, higher cholesterol, esting electrocardiographic results level 2, higher exercise induced angina, higher old peak, the slope of the peak exercise level 1, major vessels (0-3) colored by flourosopy level 1, 2, 3 are more likely to have heart disease.
    
- Odds Ratio higher than one would mean that the outcome of patients having heart disease,
```{r}
# Odds Ratios
exp(logistic_2$coefficients)
```

# Margin effect 

The marginal effect refers to the change in the probability of the outcome (i.e., the dependent variable) associated with a one-unit change in a predictor variable, holding all other variables constant. Since logistic regression models the log-odds of the outcome, the marginal effect is a way to interpret the effect of a predictor variable in terms of actual probabilities, which is more intuitive.

Marginal effects interpretation:

For individual who are male: 17.27% more likely to have heart disease
For each unit increase in resting blood pressure: 0.225% more likely to have heart disease. 
For chest pain type 1: 10% more likely to have heart disease
For chest pain type 2: 19.58% less likely to have heart disease
For chest pain type 2: 21.21% less likely to have heart disease

```{r}
# Logit model average marginal effects
LogitScalar <- mean(dlogis(predict(logistic_2, type="link")))
avg_m_effects <- LogitScalar*coef(logistic_2)
avg_m_effects
```
# McFadden's Pseudo R-squared

Pseudo R-squared is a measure of model fit in logistic regression, similar to R-squared in linear regression, but it doesn't represent the proportion of variance explained. Instead, it quantifies how well the model explains the data compared to a null model (a model with no predictors). 

McFadden's R-squared: Based on the log-likelihood ratio between the fitted model and the null model. Values closer to 1 indicate better fit.

The value implies that our model explains approximately 59.45% of the variation in the outcome variable compared to the null model. Values between 0.2 and 0.4 are often considered acceptable in practice, so a value of 0.59 suggests a very strong model fit.

Soure: "https://kapooramanpreet.github.io/documents/research/papers/Wolf,Kapoor,Hobson&Gardner-McCune_SIGCSE2023_Internships_Paper.pdf" 
McFadden, D. 1977. Quantitative Methods for Analyzing Travel Behaviour of Individuals: Some Recent Developments.

```{r}
pseudo_r2 <- pR2(logistic_2)
pseudo_r2

```

# Predicting test data set and Accuracy.



1. We use K-fold cross validation 

"In K-fold cross-validation, the data set is divided into a number of K-folds and used to assess the model's ability as new data become available. K represents the number of groups into which the data sample is divided. For example, if you find the k value to be 5, you can call it 5-fold cross-validation."

2. Here we use 10 fold

3. Around 824 samples are used in training set and remaining 103 samples are used as test set. 
4. Around 87% accuracy


3. Around 922 samples are used in training set and remaining 103 samples are used as test set. 
4. Around 87% accuracy



```{r}
# Assess model using Cross Validation

crossValSettings <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)

crossVal <- train(target ~ age + sex + cp + trestbps + chol + thalach + 

    exang + oldpeak + slope + ca + thal, data=train, family = "binomial", method = "glm", trControl = crossValSettings)

crossVal

pred <- predict(crossVal, newdata = test)
confusionMatrix(data = pred, test$target)

    exang + oldpeak + slope + ca + thal, data=mydata.clean, family = "binomial", method = "glm", trControl = crossValSettings)

crossVal

pred <- predict(crossVal, newdata = mydata.clean)
confusionMatrix(data = pred, mydata.clean$target)


```

## Model 2 - Random Forest 



```{r}
# Split the data
set.seed(123)
indexSet <-sample(2, nrow(mydata.clean), replace = T, prob = c(0.8, 0.2))
train <- mydata.clean[indexSet==1,]
test <- mydata.clean[indexSet==2,]

```


1. A very low OOB (Out-of-bag) value suggest overfitting:
2. Reducing the complexity of the random forest model by decreasing the number of trees, limiting the maximum depth of trees, or removing features.

```{r}
set.seed(1234)
rf <- randomForest(target ~ ., data=train, proximity=TRUE, importance =TRUE, do.trace = 10, ntree = 50)
rf
```
1. Error rate become relatively flat after 10 trees

```{r}
# Plot the error rate to see what is the suitable number of trees.

oob.error.data <- data.frame(
  Trees=rep(1:nrow(rf$err.rate), times=3),
  Type=rep(c("OOB", "Disease", "No_Disease"), each=nrow(rf$err.rate)),
  Error=c(rf$err.rate[,"OOB"], 
    rf$err.rate[,"Disease"], 
    rf$err.rate[,"No_Disease"]))

ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +
  geom_line(aes(color=Type))

```


1. Here we select 5 as number of tree

1. Here we select 10 as number of tree

2. OOB is around 5% means 95% samples were correctly classified by the random forest

```{r}
set.seed(1235)
rf2 <- randomForest(target ~ ., data=train, proximity=TRUE, importance =TRUE, ntree = 5)
rf2
```


# Feature Selection:

The function varImpPlot() in Random Forest regression (and classification) is used to visualize the importance of each predictor (independent variable) in the model. It helps to understand which features contribute most to the model’s predictions. This is particularly useful when you have many predictor variables and want to identify which ones are most influential.

Purpose of varImpPlot() in Random Forest Regression:
Feature Importance: varImpPlot() plots the importance of each variable in the model, allowing you to assess which variables have the greatest effect on the target variable.

Model Interpretation: By visualizing feature importance, you can identify key predictors and gain insights into your data and model. This helps in understanding the model’s behavior and making decisions about which features to retain, remove, or transform.

Dimensionality Reduction: If certain variables have very low importance, you might decide to drop them from the model to reduce complexity, improve interpretability, and potentially improve model performance.

Here we removed fbs, restecg, slope variables as they are less important.




```{r}
#Variable Importance - which variables can we remove?

varImpPlot(rf2)
```


# Final model and accessing classification accuracy

1. fbs, restecg are removed as they are less important 


```{r}
set.seed(12)
rf3 <- randomForest(target ~ age + sex + cp + trestbps + chol + thalach + 

    exang + oldpeak + ca + thal, data=train, proximity=TRUE, importance =TRUE, ntree = 5)

    exang + oldpeak + slope + ca + thal, data=train, proximity=TRUE, importance =TRUE, ntree = 5)

rf3

```

```{r}

result <- data.frame(test$target, predict(rf3, test[, 0:13], type="response"))
head(result)
```






```{r}
confusionMatrix(result$predict.rf3..test...0.13...type....response.., result$test.target)
```



## Which model can better classify?  -ROC Curve and AUC

To determine which model performs better we use AUC-ROC Curve:

AUC-ROC evaluates the ability of a model to distinguish between classes across various thresholds. A higher AUC indicates better discrimination ability.The model with the higher AUC is generally better.

## Select Mode -ROC Curve and AUC


Random Forest Model has higher AUC and below ROC Curve suggest Random Forest Model is better.

```{r}


par(pty="s")
roc(train$target, logistic_2$fitted.values, plot=TRUE, legacy.axes=TRUE, percent=TRUE,
    xlab="False Positive Percentage", ylab="True Positive Percentage", col="#0ABAB5",  print.auc=TRUE)
plot.roc(train$target, rf3$votes[,1], percent=TRUE, col="#FA8072", print.auc=TRUE, add=TRUE, print.auc.y=40)
legend("bottomright", legend=c("Logistic Regression", "Random Forest"), col=c("#0ABAB5","#FA8072"), lwd=2)


par(pty="s")
roc(mydata$target, logistic_2$fitted.values, plot=TRUE, legacy.axes=TRUE, percent=TRUE,
    xlab="False Positive Percentage", ylab="True Positive Percentage", col="#0ABAB5",  print.auc=TRUE)
plot.roc(train$target, rf3$votes[,1], percent=TRUE, col="#FA8072", print.auc=TRUE, add=TRUE, print.auc.y=40)
legend("bottomright", legend=c("Logistic Regression", "Random Forest"), col=c("#0ABAB5","#FA8072"), lwd=2)

```








